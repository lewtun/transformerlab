{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import gelu\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "from transformers import (RobertaTokenizer, PreTrainedModel, RobertaConfig, \n",
    "                          RobertaForMaskedLM, DataCollatorForLanguageModeling,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "from transformers.modeling_outputs import MaskedLMOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oscar.eo.ds  oscar.eo.txt\n"
     ]
    }
   ],
   "source": [
    "data = Path(\"data/\")\n",
    "!ls {data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merges.txt  vocab.json\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"models/esperberto\"\n",
    "!ls {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-16 09:19:52--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
      "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 99.84.114.112, 99.84.114.24, 99.84.114.120, ...\n",
      "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|99.84.114.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 312733741 (298M) [text/plain]\n",
      "Saving to: ‘data/oscar.eo.txt’\n",
      "\n",
      "data/oscar.eo.txt   100%[===================>] 298.25M  56.4MB/s    in 5.3s    \n",
      "\n",
      "2021-04-16 09:19:58 (56.3 MB/s) - ‘data/oscar.eo.txt’ saved [312733741/312733741]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c -O data/oscar.eo.txt https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ĉu ... preĝi | mediti | ricevi instigojn || kanti | muziki || informiĝi | legi | studi || prepari Diservon\n",
      "Temas pri kolekto de kristanaj kantoj, eldonita de Adolf Burkhardt inter 1974 kaj 1990 en dek kajeretoj. Ili estas reeldonitaj inter 1995 kaj 1998 de Bernhard Eichkorn en tri kajeroj, kies tria estas pliampleksigita per Dek Novaj Kantoj kaj suplemento, same de Adolf Burkhardt.\n",
      "En la dua kaj tria kajero oni adiciis 300 al la originaj kantonumeroj, por ke oni povu pli facile uzi la kajerojn kune kun la KELI-himnaro Adoru Kantante, kiu havas malpli ol 300 numerojn.\n",
      "Ni ĝojus, se iu trovus bonajn ekzemplerojn de la dek originaj kajeretoj kaj tempon por skani ankaŭ ilin. Bonvolu ekkontaktiĝi kun ni!\n",
      "Lerni Esperanton per telefono, novaĵoj Poŝtkarto 120 jaroj de fervojo Svitavy-Polička 189… T.n.migranta poŝtkarto el 1908 BK - Kongresa Biblioteko en Vaŝingtono 1- 910 BK - Nederlando- Esperanta elektra tramo en Hago (… La lernolibro \"Esperanto per rekta metodo\" jam en… IMG 7181 Nova poŝtkarto - \"Esperanto sur poŝtmarkoj kaj bil… Vizitu urbon Přerov - bildkarto Saluton el Prostějov - bildkarto Saluton el Praha - bildkarto La 115-a datreveno de E-Klubo Brno Strážnice-bildkarto Svitavy - bildkarto Pardubice - bildkarto La Balta Ondo prezentas Juna Amiko 1_2016 Nova kurso de Esperanto por saĝaj telefonoj Krucenigmoj kaj sonlibroj en Esperanto Vintra bildkarto el Finnlando Esperantisto Slovaka 4/2015 Juna Amiko 4_2015 Mia prelego dum la 3a Tutmonda Kolokvo pri Instrua… Mia prelego dum la 3a Tutmonda Kolokvo pri Instrua… Esperantisto Slovaka 3/2015, dorso /zadná strana o… Esperantisto Slovaka 3/2015, kovrilo /obálka Juna Amiko 3/2015 Esperanto Aktuell 4.2015 Esperantisto Slovaka 2/2015\n",
      "per ĝi oni povas telefoni, fari fotojn, trovi la vojon, interreti, tviteri, pririgardi filmojn, aŭskulti muzikon, ktp., ktp.\n",
      "Internacia Esperanto-Semajno de la Kulturo kaj Turismo Internacia Esperanto-Semajno de la Kulturo kaj Turismo\n",
      "''P3 KBS_E-ton'' Plikonigi, plibonigi kaj Plisciigi Esperanton ''P3 KBS_E-ton'' Plikonigi, plibonigi kaj Plisciigi Esperanton\n",
      "Kvankam la kurso mem nur estas en Esperanto ĝi por diversaj naciaj komunumoj aperis kiel libro kaj kiel kompakta disko kun nacilingvaj klarigoj.\n",
      "Apo (mallongigo de aplikaĵo) estas komputilprogramo por t.n. plurfunkciaj poŝtelefonoj (angle: smartphones).\n"
     ]
    }
   ],
   "source": [
    "!head {data/\"oscar.eo.txt\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/oscar.eo.txt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [str(x) for x in data.glob(\"**/*.txt\")]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/esperberto/vocab.json', 'models/esperberto/merges.txt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    f\"{model_dir}/vocab.json\",\n",
    "    f\"{model_dir}/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Mi estas Julien.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Mi', 'Ġestas', 'ĠJuli', 'en', '.', '</s>']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Mi estas Julien.\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-31220d7f73477105\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-31220d7f73477105/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 974616\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('text', data_files={'train': [paths[0]]})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La libera direktisto de pasvorto kaj sekretaj datumoj. La programaro uzas specialan ĉifrado algoritmo por la confidencialidad de la informo stokita.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18 sed memoru la Eternulon, vian Dion, ĉar Li estas Tiu, kiu donas al vi forton, por akiri grandan havon, por plenumi Sian interligon, pri kiu Li ĵuris al viaj patroj, kiel vi vidas nun. 19 Kaj mi avertas vin hodiaŭ, ke, se vi forgesos la Eternulon, vian Dion, kaj sekvos aliajn diojn kaj servos al ili kaj kliniĝos antaŭ ili, tiam vi pereos; 20 simile al la popoloj, kiujn la Eternulo pereigas antaŭ vi, tiel vi pereos; pro tio, ke vi ne obeos la voĉon de la Eternulo, via Dio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asquith sekvis tion per jesado teni Komisionojn de Enketo en la konduton de Dardaneloj kaj da la Mesopotamian kampanjo, kie Aliancite fortoj estis devigita kapitulaci ĉe Kut. [273] Sir Maurice Hankey, Sekretario al la Milito-Komisiono, pripensis tion; \"la koalicio neniam resaniĝis. Dum (ĝia) lasta kvin monatoj, la funkcio de la Ĉefkomando estis aranĝita sub la ombron de tiuj mortenketoj.\" [274] Sed tiuj eraroj estis ombritaj fare de la limigita progreso kaj enormaj viktimoj de la Batalo ĉe la Somme, kiu komenciĝis la 1an de julio 1916, kaj tiam per alia giganta persona perdo, la morto de la filo de Asquith Raymond, la 15an de septembro ĉe la Battle of Flers-Courcelette (Batalo de Flers-Courcelette). [275] La rilato de Asquith kun lia majoratulo ne estis facila. Raymond skribis al sia edzino frue en 1916; \"Ĉu Margot-babiladoj plu babilaĵo al vi pri la malhomeco de ŝia paŝinfanoj vi povas maldaŭrigi ŝian buŝon rakontante al ŝi ke dum mia 10 monatekzilo ĉi tie la Pm neniam skribis al mi linion de iu priskribo.\" [276] Sed la morto de Raymond estis frakasa, Viola skribo; \"... por vidi Patro-sufero tiel tordas tian\", [277] kaj Asquith pasigis multon da la sekvaj monatoj \"malparolema kaj malfacila alproksimiĝi\". [278] La Milito alportis neniun libertempon, Churchill skribanta tion; \"La malsukceso rompi la germanan linion en la Somme, la reakiro da la ĝermanaj potencoj en la orienta [i.e. la malvenko de la Brusilov Ofensivo], la ruino de Rumanio kaj la komencoj de renoviĝinta submarŝipa milito fortigis kaj stimulis ĉiujn tiujn fortojn kiuj insistis sur daŭre pli granda vigleco en la konduto de aferoj.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fotaro de Juci (Vidor Judit) estas cxe: http://jucimam.multiply.com/ fotaro de Maria Nuyanzina estas cxe http://www.flickr.com/, fotaro de Jxomart kaj Natasxa estas cxe http://www.jomart.net/bilder/ kaj fotaro de Rade Kuzmanovic estas cxi tie: http://ijs7.aim.ac.yu/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La suba korpo, kun naŭ truoj kaj ok klavoj, tenata de la dekstra mano (foje pli por la basaj tipoj);</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Petro la kultinda, kiu tradukigis la Koranon kaj la Talmudon kaj studis ilin, estis verŝajne la unua kontraŭjudulo ; li skribis ideojn kiuj utilis, en Liono, dum la kunveno (Koncilio)... ,kiu organizis inkvizicion - terura afero : arestado kaj bruligo de ne-katolikaj homoj. Abato Petro mortis en 1156.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>La agado de UEA sur E-ista kampo estis tiu de ligilo inter la ankoraŭ funkciantaj organizaĵoj en kelkaj neŭtralaj landoj kaj la aktivaj E-istoj. Aperis regule ,E‘ ĉiumonate kun artikoloj pri tutmondaj problemoj. En 1916 aperis jarlibro, kiu enhavis ĉefajn informojn kaj la adresaron de delegitoj. Plie aperis dufoje libreto: „E dum la milito“, kiu enhavis resumon de la stato de la movado. Elokvente parolis pri la sekvoj de la milito la ciferoj. En 1914 la nombro de pagintaj anoj estis 7233, en 1915 estis 2699, en 1918 1958.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pri la aktorino Z. oni diris, ke ŝi mortigis sin pro malfeliĉa amo. Sinjoro Kojno diris: ŝi mortigis sin pro amo al si mem. La X-on ŝi ĉiukaze ne povas esti aminta. Alie ŝi tion ne estus farinta al li. Amo estas la deziro ion doni, ne ricevi. Amo estas la arto ion produkti per la kapabloj de la alia. Por tio oni bezonas de la alia respekton kaj simpation. Tion oni povas ĉiam akiri. La troa deziro esti amata malmulte rilatas kun vera amo. Memamo ĉiam havas ion memmortigan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Helpu al Vikipedio plilongigi ĝin. Se jam ekzistas alilingva samtema artikolo pli disvolvita, traduku kaj aldonu el ĝi (menciante la fonton).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Post la kristanigo, Rottweil apartenis unue al la katolika diocezo Konstanco. Kiel libera regna urbo Rottweil povis ordigi la religiajn aferojn mem. La reformacio unue disvastiĝis en la urbaj gildoj sed ne estis enkondukita far la urba magistrato. Ĝis 1545 la anoj de la reformacio estis forpelitaj el la urbo. Per tio Rottweil kaj ĝiaj vilaĝoj restis katolikaj ĝis la 19-a jarcento. Ekde 1821 resp. 1827 la katolikaj komunumoj en la hodiaŭa urboregiono apartenas al la diocezo Rottenburg. Rottweil fariĝis sidejo de dekanejo. Al ĝi apartenas ĉiuj hodiaŭaj katolikaj komunumoj en la tuta urba regiono.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_dir, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2740563aedae40f9862a8a3e168e665d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=974616.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ds_enc = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_enc.save_to_disk(\"data/oscar.eo.ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'text'],\n",
       "        num_rows: 974616\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_enc = DatasetDict.load_from_disk(\"data/oscar.eo.ds\")\n",
    "ds_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Ĉu... preĝi | mediti | ricevi instigojn || kanti | muziki || informiĝi | legi | studi || prepari Diservon</s>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ds_enc[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_init():\n",
    "    return RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at data/oscar.eo.ds/train/cache-8b96f05ff2fc9096.arrow and data/oscar.eo.ds/train/cache-e43a2b26405c9c65.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>10.374800</td>\n",
       "      <td>9.909284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>9.829200</td>\n",
       "      <td>9.645468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>9.572800</td>\n",
       "      <td>9.329382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>9.437400</td>\n",
       "      <td>9.316128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ds = ds_enc[\"train\"].train_test_split(train_size=512, test_size=128, seed=42)\n",
    "bs = 8\n",
    "logging_steps = sample_ds[\"train\"].num_rows // bs // 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/esperberto\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=bs,\n",
    "    prediction_loss_only=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=baseline_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=sample_ds[\"train\"],\n",
    "    eval_dataset=sample_ds[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: implement RoBERTa LM from scratch :) Remove as much boilerplate as possible while maintaining compatibility with the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaPreTrainedModel, RobertaLMHead, RobertaLayer)\n",
    "from transformers.modeling_outputs import BaseModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
    "    \"\"\"\n",
    "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
    "\n",
    "    Args:\n",
    "        x: torch.Tensor x:\n",
    "\n",
    "    Returns: torch.Tensor\n",
    "    \"\"\"\n",
    "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "    mask = input_ids.ne(padding_idx).int()\n",
    "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
    "    return incremental_indices.long() + padding_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRobertaEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
    "    \n",
    "    LT: For some reason, removing the token_type_embeddings produces small numerical differences in the losses - safe to remove?\n",
    "    \"\"\"\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "\n",
    "        # End copy\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "\n",
    "        # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
    "        position_ids = create_position_ids_from_input_ids(\n",
    "            input_ids, self.padding_idx, past_key_values_length\n",
    "        ).to(input_ids.device)\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "\n",
    "\n",
    "        token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRobertaEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRobertaModel(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.embeddings = MyRobertaEmbeddings(config)\n",
    "        self.encoder = MyRobertaEncoder(config)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "    ):\n",
    "        input_shape = input_ids.size()\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        device = input_ids.device \n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "        )\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=sequence_output,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRobertaForMaskedLM(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.roberta = MyRobertaModel(config)\n",
    "        self.lm_head = RobertaLMHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        masked_lm_loss = loss_fct(\n",
    "            prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return MyRobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference values\n",
    "\n",
    "# Step\tTraining Loss\tValidation Loss\n",
    "# 16\t10.272600\t9.798569\n",
    "# 32\t9.720300\t9.541902\n",
    "# 48\t9.513300\t9.259138\n",
    "# 64\t9.403300\t9.244128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>10.272600</td>\n",
       "      <td>9.798569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>9.720300</td>\n",
       "      <td>9.541902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>9.513300</td>\n",
       "      <td>9.259138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>9.403300</td>\n",
       "      <td>9.244128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_trainer():\n",
    "    trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=sample_ds[\"train\"],\n",
    "    eval_dataset=sample_ds[\"test\"]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlab",
   "language": "python",
   "name": "tlab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
